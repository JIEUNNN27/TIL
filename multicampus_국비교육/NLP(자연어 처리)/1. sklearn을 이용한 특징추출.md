[toc]

# *사이킷런을 이용한 특징 추출

자연어 처리에서 특징 추출: 텍스트 데이터에서 단어나 문장들을 어떤 특징 값(수치)으로 바꿔 주는 것

- CountVectorizer :  단순히 횟수를 기준으로 특징을 추출
- TfidfVectorizer : TF-IDF라는 값을 사용해 텍스트에서 특징을 추출
- HashingVectorizer : 해시 함수를 사용



## 1. CountVectorizer

1. 사전을 구축한다
   - dictionary, vocabulary
2. 각 문장을 사전의 index로 표현
3. count 하기



**단점**

- 순서를 고려하지 않는다.
- 문장의 '의미'파악을 하지 못한다. (중의성x)

```python
#특징 추줄
from sklearn.feature_extraction.text import CountVectorizer

text_data = ['나는 배가 고프다', 
             '내일 점심 점심 뭐먹지', #점심 2번 
             '내일 공부 해야겠다', 
             '점심 먹고 공부 해야지']
```

```python
#객체 생성
count_vectorizer = CountVectorizer()

#적합 시키기
count_vectorizer.fit(text_data)

print(count_vectorizer.vocabulary_)
-----------------------------------------
{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}
```

```python
#첫번째 문장을 수치로,.,,
sentence = [text_data[0]]
print(count_vectorizer.transform(sentence).toarray())
--------------------------------------------------------------
[[1 0 1 0 0 0 1 0 0 0]]
```

```python
# toword를 사용하는 이유
vec_1 = count_vectorizer.transform(sentence)
print(vec_1.toarray())
--------------------------------------------------------------
[[1 0 1 0 0 0 1 0 0 0]]
```

```python
#4문장 모두 수치 vector로 표현된다
print(count_vectorizer.transform(text_data).toarray())
--------------------------------------------------------------
[[1 0 1 0 0 0 1 0 0 0]
 [0 0 0 1 0 1 0 2 0 0]
 [0 1 0 1 0 0 0 0 1 0]
 [0 1 0 0 1 0 0 1 0 1]]
```



### 참고사항

- 앞으로 사전의 이름을 이렇게 통일한다 : word2idx

- ```python
  word2idx = count_vectorizer.vocabulary_
  word2idx
  -----------------------------------------------------------
  {'고프다': 0,
   '공부': 1,
   '나는': 2,
   '내일': 3,
   '먹고': 4,
   '뭐먹지': 5,
   '배가': 6,
   '점심': 7,
   '해야겠다': 8,
   '해야지': 9}
  ```

- 사전은 이렇게 두개로 만드는 것이 일반적이다.

- ```python
  #단어 : 인덱스
  word2idx = count_vectorizer.vocabulary_
  
  #인덱스 : 단어
  idx2word = {v:k for (k, v) in word2idx.items()}
  ```





## 2. TfidfVectorizer

- TF (Term Frequency)
  - 특정 단어가 하나의 데이터 안에서 등장하는 횟수
- DF (Document Frequency)
  - 문서 빈도값, 특정 단어가 여러 데이터에 자주 등장하는지 알려주는 지표
  - DF값이 높을수록 덜 중요하다
- IDF
  - DF값에 역수를 취한 것
  - IDF 값이 높을수록 더 중요하다
- **TF-IDF = TF * IDF**
  - 어떤 단어가 해당 문서에 자주 등장하지만 다른 문서에는 많이 있는 단어일수록 높은 값을 가짐

```python
#특징 추줄
from sklearn.feature_extraction.text import TfidfVectorizer

text_data = ['나는 배가 고프다', 
             '내일 점심 점심 뭐먹지', #점심 2번 
             '내일 공부 해야겠다', 
             '점심 먹고 공부 해야지']
```

```python
#객체 생성
tfidf_vectorizer = TfidfVectorizer()

#적합 시키기
tfidf_vectorizer.fit(text_data)

print(tfidf_vectorizer.vocabulary_)
---------------------------------------------------------
{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}
```

```python
#첫번째 문장을 수치로,.,,
sentence = [text_data[0]]
print(tfidf_vectorizer.transform(sentence).toarray())
-------------------------------------------------------------
[[0.57735027 0.         0.57735027 0.         0.         0.
  0.57735027 0.         0.         0.        ]]
```

```python
vec_1 = tfidf_vectorizer.transform(sentence)
print(vec_1.toarray())
---------------------------------------------------------------
]
# toword를 사용하는 이유
vec_1 = tfidf_vectorizer.transform(sentence)
print(vec_1.toarray())
[[0.57735027 0.         0.57735027 0.         0.         0.
  0.57735027 0.         0.         0.        ]]
```

```python
#4문장 모두 수치 vector로 표현된다
print(tfidf_vectorizer.transform(text_data).toarray())
-----------------------------------------------------------------------
[[0.57735027 0.         0.57735027 0.         0.         0.
  0.57735027 0.         0.         0.        ]
 [0.         0.         0.         0.3889911  0.         0.49338588
  0.         0.7779822  0.         0.        ]
 [0.         0.52640543 0.         0.52640543 0.         0.
  0.         0.         0.66767854 0.        ]
 [0.         0.43779123 0.         0.         0.55528266 0.
  0.         0.43779123 0.         0.55528266]]

```





## 3. HasingVectorizer

- 해쉬함수를 이용한다

```python
from sklearn.feature_extraction.text import HashingVectorizer
```

```python
text_data = ['나는 배가 고프다', 
             '내일 점심 점심 뭐먹지', #점심 2번 
             '내일 공부 해야겠다', 
             '점심 먹고 공부 해야지']
```

```python
#객체 생성
hash_vectorizer = HashingVectorizer(n_features = 10)

#적합 시키기
hash_vectorizer.fit(text_data)

#print(hash_vectorizer.vocabulary_)
---------------------------------------------------------
HashingVectorizer(n_features=10)
```

```python
#첫번째 문장을 수치로,.,,
sentence = [text_data[0]]
print(hash_vectorizer.transform(sentence).toarray())
---------------------------------------------------------
[[ 0.          0.57735027  0.          0.          0.57735027  0.
  -0.57735027  0.          0.          0.        ]]
```



