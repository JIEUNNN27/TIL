[toc]

학습 기반 모델

- NLPM
- Word2Vec
- FastText

행렬 분해 기반

- LSA
- Glove - 학습 + 행렬분해
- Swivel - 학습 + 행렬분해



# 1. NPLM 

(Neural Probabilistic Language Model)

- 자연어 처리 분야에 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델
- 다음 단어 예측(단방향)



## 1.1 모델 기본 구조

요수아 벤지오 연구 팀이 제안한 기법(Bengio et al.,2003)

통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 탄생함



\* Bengio가 정리한 기존 언어 모델(Neural n-gram)의 단점

- 학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타날 확률 값을 0으로 부여.
  - 백오프(back-off)나 스무딩(smoothing)으로 이런 문제를 일부 보완할 수 있지만 완전한 것은 아님
- 문장의 장기 의존성(long-term dependency)을 포착하기 어렵다. 다시 말해 n-gram 모델의 n을 5 이상으로 길게 설정할 수 없다
- 단어/문장 간 유사도를 계산할 수 없다



NPLM은 이러한 기존 언어 모델의 한계를 일부 극복한 언어 모델이라는 점에서 의의가 있다

- 그 뿐만 아니라 **NPLM 자체가 단어 임베딩 역할을 수행할 수 있다.**



![Neural Probabilistic Language Model · ratsgo's blog](http://i.imgur.com/vN66N2D.png)



## 1.2 NPLM의 학습

NLPM은 단어 시퀀스가 주어졌을 때 다음 단어가 무엇인지 맞추는 과정에서 학습됨.

**NLPM은 직전까지 등장한 n-1개 단어들로 다음 단어를 맞추는 n-gram 언어 모델**



### NPLM의 출력

![image-20220324143805561](C:\Users\silvi\AppData\Roaming\Typora\typora-user-images\image-20220324143805561.png)

NPLM 구조 말단의 출력은 |V|차원의 스코어 벡터 Ywt에 **소프트맥스(softmax)** 함수를 적용한 |V|차원의 확률 벡터



### NPLM의 입력

![image-20220324144219494](C:\Users\silvi\AppData\Roaming\Typora\typora-user-images\image-20220324144219494.png)

문장 내 t번째 단어(wt)에 대응하는 단어 벡터 xt를 만드는 과정.

|V|*m 크기를 갖는 커다란 행렬 C에서 wt에 해당하는 벡터를 *참조(lookup)*\*한 형태.

- |V| : 어휘 집합 크기
- m : xt의 차원 수
- C행렬의 원소값은 초기에 랜덤 설정한다.

![image-20220324145516916](C:\Users\silvi\AppData\Roaming\Typora\typora-user-images\image-20220324145516916.png)

위의 수식처럼 각각의 단어에 해당하는 열 벡터를 C에서 참조한 뒤, 이 벡터들을 묶어주면(concatenate)  NPLM으 입력 벡터 x가 된다. 

- n은 고려 대상 n-gram의 개수



### \*참조의 의미

C(wt)는 행렬 C와 wt에 해당하는 원핫벡터(One-hot-Vector)를 내적(inner product)한 것과 같다.

- 원핫벡터(One-hot-Vector) : 한 요소만 1이고 나머지는 0인 벡터를 가리킨다.
- 이는 C라는 행렬에서 wt에 해당하는 행(row)만 참조하는 것과 동일



### NPLM의 연산

Input layer를 통해 나온 x가 Hidden layer와 Output layer를 거치는 연산.

![image-20220324145914380](C:\Users\silvi\AppData\Roaming\Typora\typora-user-images\image-20220324145914380.png)

- b, d : bias term
- H : Hidden layer의 weights
- W : input layer와 output layer의 direct connection을 만들 경우의 weights



### NPLM의 학습

마지막으로 ywt에 소프트맥스 함수를 적용한 뒤 이를 정답 단어의 인덱스와 비교해 역전파 하는 방식으로 학습이 이루어 진다.

NPLM 학습이 종료되면 행렬 C를 각 단어에 해당하는 m차원 임베딩으로 사용한다.



## 1.3 NPLM과 의미 정보

NPLM은 단어의 의미를 어떻게 임베딩에 녹여낼 수 있는 걸까?

```
#bengio 연구 팀이 예로 든 유사 문장

The cat is walking in the bedroom
A dog was running in a room
The cat is running in a room
A dog is walking in a bedroom
The dog was walking in the room
```

n-gram의 n을 4로 두고 생각. 그렇다면 NPLM은 직전 3개 단어를 가지고 그 다음 단어 하나를 맞추는 과정에서 학습됨.

The, A, cat, dog, is was 등에 해당하는 C 행렬의 행 벡터들은  walking을 맞추는 과정에서 발생한 학습 손실(train loss)을 최소화하는 그래디언트(gradient)를 받아 동일하게 업데이트된다.

- 결과적으로, The, A, cat, dog, is, was 벡터가 벡터 공간에서 같은 방향으로 조금 움직인다는 이야기



NPLM은 그 자체로 언어 모델 역할을 수행할 수 있다(Next 단어 예측)

- 기존의 통계기반 n-gram 모델은 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 그 등장 확률을 0으로 부여하는 문제점을 가지고 있다.
- 하지만 NPLM은 문장이 말뭉치에 없어도 문맥이 비숫한 다른 문장을 참고해 확률을 부여한다









책: 이기창, 한국어 임베딩, 에이콘, 2020

논문: [A Neural Probabilistic Language Model, 2003 Bengio et al](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)



# 2. Word2Vec (CBOW, Skip-gram)

- 2013년 구글 연구팀이 발표한 기법, 가장 널리 쓰이고 있는 단어 임베딩 기법
- 두가지 논문
  - 논문 1: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
    - Skip-Gram과 CBOW라는 모델을 제안
  - 논문 2 : [Distributed Representations of Words and Phrase and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)
    - Negative sampling등 학습 최적화 기법을 제안



## 2.1 모델 기본 구조

![NLP 101: Word2Vec — Skip-gram and CBOW | by Ria Kulshrestha | Towards Data  Science](https://miro.medium.com/max/1400/1*cuOmGT7NevP9oJFJfVpRKA.png)

### CBOW

- 주변에 있는 문맥 단어(context word)들을 가지고 타깃 단어(target word) 하나를 맞추는 과정에소 학습됨.

### Skip-gram

- 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정에서 학습됨
- 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향이 있다



## 2.2 학습 데이터 구축

### 포지티브 샘플(positive sample)

- 타깃 단어(t)와 그 주변에 실제로 등장한 문맥 단어(c) 쌍

### 네거티브 샘플(negative sample)

- 타깃 단어와 그 주변에 등장하지 않은 단어(말뭉치 전체에서 랜덤 추출) 쌍



### Skip-gram

- 전체 말뭉치를 단어별로 슬라이딩해 가면서 학습 데이터를 만든다.
- **Skip-gram 모델은 같은 말뭉치를 두고도 엄청나게 많은 학습 데이터 쌍을 만들어낼 수 있다**



논문1

- 타깃 단어가 주어졌을 때 문맥 단어가 무엇인지 맞추는 과정에서 학습됨.
- 하지만 이방식은 **소프트맥스** 때문에 계산량이 비교적 큰 편, 따라서 모두 계산 하려면 비효율적

논문2

- 네거티브 샘플링(negative sampling)
  - 타깃 단어와 문맥 단어가 주어졌을 때 해당 쌍이 포지티브 샘플(+)인지, 네거티브 샘플(-)인지 이진 분료('binary classification') 하는 과정에서 학습됨
  - 1개의 포지티브 샘플과 k개의 네거티브 샘플만 계산하면 되기 때문에 계산량이 훨씬 적다
    - (정확히는 스텝마다 차원 수가 2인 시그모이드를 k+1회 계산)
  - 작은 데이터에는 k를 5~20, 큰 말뭉치에는 k를 2~5로 하는 것이 성능이 좋았다

- 서브샘플링(subsampling)
  - 자주 등장하는 단어는 학습에서 제외한다



## 2.3 모델 학습

