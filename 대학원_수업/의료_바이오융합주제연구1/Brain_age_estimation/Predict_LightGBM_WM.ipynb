{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a610081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, f_regression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c46c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% option\n",
    "option = {}\n",
    "option['seed'] = 42\n",
    "option['applyfeatureselection'] = True\n",
    "option['thresholdpvalue'] = 0.05\n",
    "option['cvfoldnum'] = 5\n",
    "option['cvrepeatnum'] = 1\n",
    "#option['crange'] = np.logspace(-4,6,num=11)\n",
    "option['crange'] = np.logspace(-3,3,num=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529e6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],  # 트리 개수\n",
    "    'max_depth': [3, 6, 9, None],  # 트리의 최대 깊이\n",
    "    'learning_rate': [0.01, 0.1, 0.5],  # 학습률\n",
    "    'subsample': [0.6, 0.8, 1.0],  # 샘플링 비율\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]  # 트리 구성에 사용될 피처 비율\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ea3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% xtxtfile\n",
    "datapath = './' # specify the path where the 'data_regression' directory exists\n",
    "machinelearningpath = os.path.join(datapath, 'machine_learning3')\n",
    "xtxtfile = {}\n",
    "xtxtfile['train'] = os.path.join(machinelearningpath, 'X_train.txt')\n",
    "xtxtfile['test'] = os.path.join(machinelearningpath, 'X_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a375aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% variable\n",
    "variable = {}\n",
    "variable['response'] = 'Age'\n",
    "variable['confounding'] = ['TIV', 'Sex']\n",
    "variable['predictor'] = 'WM'\n",
    "variable['categorical'] = ['Sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1c191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% modelpath\n",
    "if option['applyfeatureselection']:\n",
    "    modelpath = os.path.join(machinelearningpath, f\"{variable['predictor']}_WithFS_{option['cvrepeatnum']}\\u00D7{option['cvfoldnum']}CV\")\n",
    "else:\n",
    "    modelpath = os.path.join(machinelearningpath, f\"{variable['predictor']}_WithoutFS_{option['cvrepeatnum']}\\u00D7{option['cvfoldnum']}CV\")\n",
    "if not os.path.exists(modelpath):\n",
    "    os.makedirs(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e07b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% X, y\n",
    "T = pd.read_csv(xtxtfile['train'], delimiter=',')\n",
    "y_train = T[variable['response']]\n",
    "X_train = T.drop(columns=[variable['response']])\n",
    "X_test = pd.read_csv(xtxtfile['test'], delimiter=',')\n",
    "feature = list(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8168bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 진행 상황 표시를 위한 tqdm 객체 생성\n",
    "progress_bar = tqdm(total=option['cvrepeatnum'] * option['cvfoldnum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd227240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% summarytxtfile\n",
    "summarytxtfile = os.path.join(modelpath, 'Summary.txt')\n",
    "fid = open(summarytxtfile, 'wt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0150c1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:12<00:00, 26.59s/it]\n"
     ]
    }
   ],
   "source": [
    "#%% cv\n",
    "cvnum = option['cvfoldnum'] * option['cvrepeatnum']\n",
    "featureweight = np.full([cvnum, len(feature)],np.nan)\n",
    "T_featureweight = pd.DataFrame(index=feature)\n",
    "performance = {\"mae\": np.empty(cvnum), \"rmse\": np.empty(cvnum)}\n",
    "y_predict = np.empty([cvnum, X_test.shape[0]])\n",
    "random.seed(option['seed'])\n",
    "for i_repeat in range(option['cvrepeatnum']):\n",
    "    cv = KFold(n_splits=option['cvfoldnum'], shuffle=True)\n",
    "    sampleidx = {}\n",
    "    for i_fold, (sampleidx['train'], sampleidx['test']) in enumerate(cv.split(X_train, y_train)):\n",
    "        i_cv = (i_repeat * option['cvfoldnum']) + i_fold + 1\n",
    "        print(f'-------------------- CV {i_cv} --------------------',file=fid)\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # X_cv, y_cv\n",
    "        X_cvtrain, y_cvtrain = X_train.iloc[sampleidx['train']], y_train.iloc[sampleidx['train']]\n",
    "        X_cvtest, y_cvtest = X_train.iloc[sampleidx['test']], y_train.iloc[sampleidx['test']]\n",
    "        X_test2 = X_test.copy()\n",
    "\n",
    "        # sacler\n",
    "        scaler = StandardScaler()\n",
    "        X_cvtrain_array = np.empty(X_cvtrain.shape)\n",
    "        X_cvtest_array = np.empty(X_cvtest.shape)\n",
    "        X_test2_array = np.empty(X_test2.shape)\n",
    "        for i_feature in range(len(feature)):\n",
    "            if feature[i_feature] not in variable['categorical']:\n",
    "                X_cvtrain_array[:, i_feature] = scaler.fit_transform(X_cvtrain.iloc[:, i_feature].values.reshape(-1, 1)).ravel()\n",
    "                X_cvtest_array[:, i_feature] = scaler.transform(X_cvtest.iloc[:, i_feature].values.reshape(-1, 1)).ravel()\n",
    "                X_test2_array[:, i_feature] = scaler.transform(X_test2.iloc[:, i_feature].values.reshape(-1, 1)).ravel()\n",
    "            else:\n",
    "                X_cvtrain_array[:, i_feature] = X_cvtrain.iloc[:, i_feature]\n",
    "                X_cvtest_array[:, i_feature] = X_cvtest.iloc[:, i_feature]\n",
    "                X_test2_array[:, i_feature] = X_test2.iloc[:, i_feature]\n",
    "        X_cvtrain = pd.DataFrame(X_cvtrain_array,columns=X_cvtrain.columns,index=X_cvtrain.index)\n",
    "        X_cvtest = pd.DataFrame(X_cvtest_array,columns=X_cvtest.columns,index=X_cvtest.index)\n",
    "        X_test2 = pd.DataFrame(X_test2_array,columns=X_test2.columns,index=X_test2.index)\n",
    "        del scaler, X_cvtrain_array, X_cvtest_array, X_test2_array\n",
    "\n",
    "        # selector\n",
    "        if option['applyfeatureselection']:\n",
    "            selector = GenericUnivariateSelect(f_regression)\n",
    "            selector.fit(X_cvtrain,y_cvtrain)\n",
    "            featureidx_selected = selector.pvalues_ <= option['thresholdpvalue']\n",
    "            X_cvtrain = X_cvtrain.iloc[:,featureidx_selected]\n",
    "            X_cvtest = X_cvtest.iloc[:,featureidx_selected]\n",
    "            X_test2 = X_test2.iloc[:,featureidx_selected]\n",
    "            del selector\n",
    "        else:\n",
    "            featureidx_selected = range(len(feature))\n",
    "        print(f'Training for {X_cvtrain.shape[0]} samples \\u00D7 {X_cvtrain.shape[1]} features',file=fid)\n",
    "        print(f'Test for {X_cvtest.shape[0]} samples \\u00D7 {X_cvtest.shape[1]} features',file=fid)\n",
    "\n",
    "        # xgboost\n",
    "        mdl = lgb.LGBMRegressor()\n",
    "        searcher = GridSearchCV(mdl, param_grid=param_grid, scoring='neg_mean_absolute_error')\n",
    "        searcher.fit(X_cvtrain, y_cvtrain)\n",
    "        mdl = searcher.best_estimator_\n",
    "        #featureweight[i_cv-1, featureidx_selected] = mdl.coef_\n",
    "        T_featureweight[f\"CV{i_cv}\"] = featureweight[i_cv-1, :]\n",
    "        performance[\"mae\"][i_cv-1] = mean_absolute_error(y_cvtest, mdl.predict(X_cvtest))\n",
    "        performance[\"rmse\"][i_cv-1] = mean_squared_error(y_cvtest, mdl.predict(X_cvtest))**0.5\n",
    "        y_predict[i_cv-1, :] = mdl.predict(X_test2)\n",
    "        print(f\"MAE = {performance['mae'][i_cv-1]:.3f}\",file=fid)\n",
    "        print(f\"RMSE = {performance['rmse'][i_cv-1]:.3f}\",file=fid)\n",
    "        del i_cv, X_cvtrain, y_cvtrain, X_cvtest, y_cvtest, X_test2, featureidx_selected, searcher, mdl\n",
    "    del cv, sampleidx\n",
    "\n",
    "#%% avearage\n",
    "print('-------------------- Average --------------------',file=fid)\n",
    "featureweight_average = np.mean(featureweight, axis=0)\n",
    "T_featureweight['Average'] = featureweight_average\n",
    "T_featureweight.to_excel(os.path.join(modelpath, 'FeatureWeight.xlsx'), index=True)\n",
    "featureidx_common = ~np.isnan(featureweight_average)\n",
    "print(f'{np.count_nonzero(featureidx_common)}/{len(feature)} features commonly used',file=fid)\n",
    "print(f\"MAE = {np.mean(performance['mae']):.3f}\\u00B1{np.std(performance['mae']):.3f}\",file=fid)\n",
    "print(f\"RMSE = {np.mean(performance['rmse']):.3f}\\u00B1{np.std(performance['rmse']):.3f}\",file=fid)\n",
    "np.savetxt(os.path.join(modelpath, 'y_predict.txt'), y_predict)\n",
    "fid.close()\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f02163",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_df = pd.DataFrame(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f12f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_df = pd.DataFrame(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd67714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_df.to_csv('./y_predict_WM_LightGBM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c546ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
